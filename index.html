<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Conference on Robot Learning (CoRL) 2023</span>
              <br>
              <br>
              <a href=""></a>
              <a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a></span>*<sup>1,4</sup>, 
              <a href="https://www.linkedin.com/in/sharonleeyen">Sharon Lee</a>*<sup>1</sup>, 
              <a href="https://mj-hwang.github.io/">Minjune Hwang</a>*<sup>1</sup>, 
              <a href="https://misoshiruseijin.github.io/">Ayano Hiranaka</a>*<sup>2</sup>, 
              <a href="https://www.chenwangjeremy.net/">Chen Wang</a><sup>1</sup>, 
              <br> 
              <a href="https://wensi-ai.github.io/">Wensi Ai</a><sup>1</sup>,   
              <a href="https://www.linkedin.com/in/ryantjj">Jin Jie Ryan Tan</a><sup>1</sup>, 
              <a href="https://www.linkedin.com/in/shreya-gupta-08">Shreya Gupta</a><sup>1</sup>, 
              <a href="https://yih301.github.io/">Yilun Hao</a><sup>1</sup>, 
              <a href="https://www.gabrael.io/">Gabrael Levine</a><sup>1</sup>, 
              <br>             
              <a href="https://ruohangao.github.io/">Ruohan Gao</a><sup>1</sup>, 
              <a href="https://psychology.stanford.edu/people/anthony-norcia">Anthony Norcia</a><sup>3</sup>, 
              <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a><sup>1,4</sup>, 
              <a href="https://jiajunwu.com/">Jiajun Wu</a><sup>1,4</sup> 
              <br>
              <sup>1</sup>Department of Computer Science, Stanford University <br>
              <sup>2</sup>Department of Mechanical Engineering, Stanford University <br>
              <sup>3</sup>Department of Psychology, Stanford University <br>
              <sup>4</sup>Institute for Human-Centered AI (HAI), Stanford University  <br>
              *Equally contributed
            </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=eyykI3UIHa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/noir_videos/sukiyaki_header.mp4"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
          </h2>
        </div>
      </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-left">
        We present Neural Signal Operated Intelligent Robots (NOIR), a general-purpose, intelligent brain-robot interface system that enables humans to command robots to perform everyday activities through brain signals. Through this interface, humans communicate their intended objects of interest and actions to the robots using electroencephalography (EEG). Our novel system demonstrates success in an expansive array of 20 challenging, everyday household activities, including cooking, cleaning, personal care, and entertainment. The effectiveness of the system is improved by its synergistic integration of robot learning algorithms, allowing for NOIR to adapt to individual users and predict their intentions. Our work enhances the way humans interact with robots, replacing traditional channels of interaction with direct, neural communication.
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-left">
          Envisioned by scientists, engineers, and artists, brain-robot interface (BRI) stands out as a thrilling but challenging research topic. 
          We want to leverage recent progress in machine learning, neuroscience, and robot learning to build novel BRI systems for:
          <br>
          <img src="./static/noir_images/motivation.png">
        </div>
      </div>
    </div>

    <!-- Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The NOIR System</h2>
        <div class="content has-text-left">
          NOIR has two components, a modular pipeline for decoding goals from human brain
            signals, and a robotic system with a library of primitive skills. The robots possess the ability to learn
            to predict human intended goals hence reducing the human effort required for decoding. 
          <br><br>
          <img src="./static/noir_images/method.png">
          <br><br>
          
          <center><h3 class="title is-5">Decoding Human Intention from EEG</h3></center>
           NOIR uses a modular pipeline for decoding human intended goals from EEG signals: (a) What object
           to manipulate, decoded from SSVEP signals using CCA classifiers; (b) How to interact with the
           object, decoded from MI signals using CSP+QDA algorithms; (c) Where to interact, decoded from
           MI signals. A safety mechanism that captures muscle tension from jaw clench is used to confirm or
           reject decoding results.<br><br>
          <img src="./static/noir_images/decoding.png">
          <br><br>
          
          <center><h3 class="title is-5">Robots with Parametrized Primitive Skills</h3></center>
          Human intention can be mapped to 14 parametrized robot skills, such as Pick(x,y,z), Place(x,y,z), and Push(x,y,z,d). Humans find novel use and combine these skills to accomplish hard tasks.
          <br><br>
          <img src="./static/noir_images/pick.gif" width="250">
          <img src="./static/noir_images/place.gif" width="250">
          <img src="./static/noir_images/push.gif" width="250">
        </div>
      </div>
    </div>

    <!-- Robot Learning. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-5">Robot Learning for more Efficient BRI</h3>
        <div class="content has-text-left">
          Decoding human intention is time-consuming and costly. The robot learns human object, skill, and parameter selections in a few-shot manner, so human effort and time can be reduced as they perform the same task in similar contexts.
          <br><br>
          Our retrieval-based few-shot object and skill selection model is shown below. It learns a latent representation for observations. Given a new observation, it finds the most relevant experience in the memory and selects the corresponding skill and object.      
          <br><br>
          <img src="./static/noir_images/os_learning.png">
          <br><br>
          Our one-shot skill parameter learning algorithm is shown below. It finds a semantically corresponding point in the test image given a reference point in the training image. The feature visualization shows 3 of the 768 DINOv2 tokens used.
          <br><br>
          <img src="./static/noir_images/param_learning.png">
        </div>
        </div>
      </div>

    <!--/ Pipeline. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments and Results</h2>
        <div class="content has-text-left">
        With NOIR, 3 human participants (2M1F) accomplished 20 long-horizon tasks (4-15 skills) everyday activities. 
        16 activities (No.2 - No.17) are tabletop manipulation tasks with Franka, and 4 (No.18 - No.21) are mobile manipulation tasks with Tiago. The tasks include 8 meal preparation tasks,
        6 cleaning tasks, 3 personal care tasks, and 3 entertainment tasks. <br><br>
        <img src="./static/noir_images/pull.png">
        </div>
      </div>
    </div>

    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h3 class="title is-5">Task Videos (8x speed, decoding period omitted)</h3>

    </div>
    </div>

  <div class="columns is-centered has-text-centered">
      <div class="content">
        <video width="300" height="240" autoplay loop muted>
          <source src="./static/noir_videos/franka_spill.mp4" type="video/mp4">
        </video>
        <div class="subtitle">WipeSpill</div>
      </div>
      &nbsp;

      <div class="content">
        <video width="300" height="240" autoplay loop muted>
          <source src="./static/noir_videos/franka_toy.mp4" type="video/mp4">
        </video>
        <div class="subtitle">CollectToy</div>
      </div>
      &nbsp;

      <div class="content">
        <video width="300" height="240" autoplay loop muted>
          <source src="./static/noir_videos/franka_trash.mp4" type="video/mp4">
        </video>
        <div class="subtitle">SweepTrash</div>
      </div>
  </div>
  

  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_book.mp4" type="video/mp4">
      </video>
      <div class="subtitle">CleanBook</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_iron.mp4" type="video/mp4">
      </video>
      <div class="subtitle">IronCloth</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_basket.mp4" type="video/mp4">
      </video>
      <div class="subtitle">OpenBasket</div>
    </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_tea.mp4" type="video/mp4">
      </video>
      <div class="subtitle">PourTea</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_table.mp4" type="video/mp4">
      </video>
      <div class="subtitle">SetTable</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_cheese.mp4" type="video/mp4">
      </video>
      <div class="subtitle">GrateCheese</div>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_banana.mp4" type="video/mp4">
      </video>
      <div class="subtitle">CutBanana</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_pasta.mp4" type="video/mp4">
      </video>
      <div class="subtitle">CookPasta</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_sandwich.mp4" type="video/mp4">
      </video>
      <div class="subtitle">Sandwich</div>
    </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_hockey.mp4" type="video/mp4">
      </video>
      <div class="subtitle">Hockey</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_gift.mp4" type="video/mp4">
      </video>
      <div class="subtitle">OpenGift</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="300" height="240" autoplay loop muted>
        <source src="./static/noir_videos/franka_tictactoe.mp4" type="video/mp4">
      </video>
      <div class="subtitle">TicTacToe</div>
    </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="450" height="300" autoplay loop muted>
        <source src="./static/noir_videos/tiago_trash.mp4" type="video/mp4">
      </video>
      <div class="subtitle">TrashDisposal</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="450" height="300" autoplay loop muted>
        <source src="./static/noir_videos/tiago_covid.mp4" type="video/mp4">
      </video>
      <div class="subtitle">CovidCare</div>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="content">
      <video width="455" height="300" autoplay loop muted>
        <source src="./static/noir_videos/tiago_plant.mp4" type="video/mp4">
      </video>
      <div class="subtitle">WaterPlant</div>
    </div>
    &nbsp;

    <div class="content">
      <video width="450" height="300" autoplay loop muted>
        <source src="./static/noir_videos/tiago_dog.mp4" type="video/mp4">
      </video>
      <div class="subtitle">PetDog (1x)</div>
    </div>
  </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-5">Quantative Results</h3>
        <div class="content has-text-centered">
          <div class="content has-text-left">
            On average, each task requires 1.8 attempts to success, and task completion time is 20.3 minutes.
            Task horizon is the average number of primitive skills executed. # attempts indicate the average number of attempts until the first success (1 means success on the first
            attempt). Time indicates the task completion time in successful trials. Human time is the percentage
            of the total time spent by human users, this includes decision-making time and decoding time.
            <img src="./static/noir_images/table1.png" width="900">
          </div>

          <div class="content has-text-left">
          Decoding accuracy at different stages of the experiment. Decoding time and accuracy, like in almost all BRI research, are the key challenges.
          <img src="./static/noir_images/table2.png" width="700">

          </div>    

          <div class="content has-text-left">
          With robot learning algorithms, object and skill selection learning reduces the decoding time by 60%, parameter learning decreases cursor movement distance by 41%.
          <br><br>
          <center><img src="./static/noir_images/disttime3.png" width="500"></center>

        </div>   

          


        </div>
      </div>
    </div>

    <!-- FAQ. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">FAQs about BRI and NOIR</h2>
        <div class="content has-text-left">
          <p><strong>
            Is EEG safe to use? Are there any potential risks or side effects of using the EEG for extended periods of time? 
          </strong></p>
          <p>
            EEG devices are generally safe with no known side effects and risks, especially when compared to invasive devices like implants.  We use saline solution to lower electrical impedance and improve conductance. The solution could cause minor skin irritation when the net is used for extended periods of time, hence we mix the solution with baby shampoo to mitigate this. 
          </p>
          <p><strong>
            How does the system ensure user safety, particularly in the context of real-world tasks with varying environments and unpredictable events?
          </strong></p>
          <p>
            On top of our 100% decoding accuracy, we implement an EEG-controlled safety mechanism to confirm or interrupt robot actions with muscle tension, as decoded through clenching. Nevertheless, it is important to note that the current implementation entails a 500ms delay when interrupting robot actions which might lead to a potential risk in more dynamic tasks. With more training data using a shorter decoding window, the issue can be potentially mitigated.
          </p>
          <p>
            Can EEG / NOIR be applied to different people? Given that the paper has only been tested on three human subjects, how can the authors justify the generalizability of the findings?
          </strong></p>
          <p>
            The EEG device employed in our research is versatile, catering to both adults and children as young as five years old. Accompanied by SensorNets of varying sizes, the device ensures compatibility with different head dimensions. Our decoding methods have been thoughtfully designed with diversity and inclusion in mind, drawing upon two prominent EEG signals: steady-state visually evoked potential and motor imagery. These signals have exhibited efficacy across a wide range of individuals. However, it is important to acknowledge that the interface of our system, NOIR, is exclusively visual in nature, rendering it unsuitable for individuals with severe visual impairments.
          </p>
          <p><strong>
            Can EEG be used outside the lab?
          </strong></p>
          <p>
            While mobile EEG devices offer portability, it is worth noting that they often exhibit a comparatively much lower signal-to-noise ratio. Various sources contribute to the noise present in EEG signals, including muscle movements, eye movements, power lines, and interference from other devices. These sources of noise exist in and outside of the lab; consequently, though we've chosen to implement robust decoding techniques based on classical statistics, more robust further filtering techniques to mitigate these unwanted artifacts and extract meaningful information accurately are needed for greater success in more chaotic environments.
          </p>
          <p><strong>
            How does the system differentiate between intentional brain signals for task execution and other unrelated brain activity? How will you address potential issues of privacy and security?
          </strong></p>
          <p>
            The decoding algorithms employed in our study were purposefully engineered to exclusively capture task-relevant signals, ensuring the exclusion of any extraneous information. Adhering to the principles of data privacy and in compliance with the guidelines set by the Institutional Review Board (IRB) for human research, the data collected from participants during calibration and experimental sessions were promptly deleted following the conclusion of each experiment. Only the decoded signals, stripped of any identifying information, were retained for further analysis.
          </p>
          <p><strong>
            How scalable is the robotics system? Can it be easily adapted to different robot platforms or expanded to accommodate a broader range of tasks beyond the 20 household activities tested? 
          </strong></p>
          <p>
            Within the context of our study, two notable constraints are the speed of decoding and the availability of primitive skills. The former restricts the range of tasks to those that do not involve time-sensitive and dynamic interactions, such as capturing a moving object. However, the advancement in decoding accuracy and the reduction of the decoding window duration may eventually address this limitation. These improvements can potentially be achieved through the utilization of larger training datasets and the implementation of machine-learning-based decoding models, leveraging the high temporal resolution offered by EEG.
          </p>
          <p><strong>
            The development of a comprehensive library of primitive skills stands as a long-term objective in the field of robotics research. This entails creating a repertoire of fundamental abilities that can be adapted and combined to address new tasks. Additionally, our findings indicate that human users possess the ability to innovate and devise novel applications of existing skills to accomplish tasks, akin to the way humans employ tools. 
          </strong></p>
          <p>
            How exactly do both individuals with and without disabilities benefit from this BRI system? 
          </p>
          <p><strong>
            The potential applications of systems like NOIR in the future are vast and diverse. One significant area where these systems can have a profound impact is in assisting individuals with disabilities, particularly those with mobility-related impairments. By enabling these individuals to accomplish Activities of Daily Living and Instrumental Activities of Daily Living[1] tasks, such systems can greatly enhance their independence and overall quality of life. 
          </strong></p>
          <p>
            Currently, individuals without disabilities may initially find the BRI pipeline to have a learning curve, resulting in inefficiencies compared to their own performance in daily activities in their first few attempts. However, robot learning methods hold the promise of addressing these inefficiencies over time, and enable robots to help their users when needed. 
          </p>
        </div>
      </div>
    </div>
    <!--/ FAQ. -->

</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{lee2023noir,
  title={NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities},
  author={Zhang, Ruohan and Lee, Sharon and Hwang, Minjune and Hiranaka, Ayano and Wang, Chen and Ai, Wensi and Tan, Jin Jie Ryan and Gupta, Shreya and Hao, Yilun and Levine, Gabrael and Gao, Ruohan and Norcia, Anthony and Fei-Fei, Li and Wu, Jiajun},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
